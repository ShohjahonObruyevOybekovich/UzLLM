# ğŸ‡ºğŸ‡¿ uzLLM â€” Train Your Own Uzbek Large Language Model  
> ğŸ§  Fine-tune, instruct, and deploy Uzbek AI assistants with HuggingFace Transformers.

<div align="center">
  <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="120" />
  <br/><br/>
  <strong>Uzbek-first LLM training pipeline</strong><br/>
  Built on ğŸ¤— Transformers Â· âš¡ PyTorch Â· ğŸ§ª Prompt Engineering Â· ğŸš€ Accelerate
</div>

---

## ğŸ“Œ What is uzLLM?

`uzLLM` is a lightweight but powerful training framework for **fine-tuning open-source LLMs (like Mistral, LLaMA, Falcon)** on **Uzbek datasets** â€” enabling full control over instruction-following AI, local assistant models, or even Uzbek GPT-style chatbots.

---

## âœ¨ Features

âœ… Fine-tune HuggingFace LLMs on Uzbek text  
âœ… Instruction-style prompt formatting (OpenAssistant style)  
âœ… Supports Mistral / LLaMA / Falcon / Qwen (any causal LM)  
âœ… Multi-GPU or Colab-compatible  
âœ… Clean conversational chat history handling  
âœ… Easy inference API via CLI or Streamlit (optional)

---

## ğŸ› ï¸ Quickstart

```bash
git clone https://github.com/your-username/uzllm.git
cd uzllm
pip install -r requirements.txt
Prepare your dataset in .jsonl format:

json
Copy
Edit
{"instruction": "Toshkent qaerda joylashgan?", "response": "Toshkent OÊ»zbekiston poytaxti."}
Start fine-tuning:

bash
Copy
Edit
python train.py \
  --model_name_or_path NousResearch/Nous-Hermes-2-Mistral-7B-DPO \
  --train_file data/uzbek-instructions.jsonl \
  --output_dir models/uzllm-mistral \
  --per_device_train_batch_size 2 \
  --num_train_epochs 3 \
  --fp16
ğŸ§ª Example Prompt Format
Your data should follow the [INST] style used in LLaMA/Mistral:

text
Copy
Edit
<s>[INST] <<SYS>>
Siz foydalanuvchiga oâ€˜zbek tilida aqlli, aniq va foydali javoblar berasiz.
<</SYS>>

Salom! Bugun ob-havo qanday?
[/INST]
ğŸ“¦ Inference
python
Copy
Edit
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("models/uzllm-mistral")
model = AutoModelForCausalLM.from_pretrained("models/uzllm-mistral")

prompt = "<s>[INST] Salom, sen kimsan? [/INST]"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids
output = model.generate(input_ids, max_new_tokens=100)
print(tokenizer.decode(output[0], skip_special_tokens=True))
ğŸ§° Tech Stack
ğŸ¤— Transformers

ğŸ§¨ Datasets

ğŸ§ª Accelerate

ğŸ§  PyTorch

ğŸ—‚ï¸ Project Structure
bash
Copy
Edit
uzllm/
â”œâ”€â”€ data/                  # Uzbek training data (jsonl)
â”œâ”€â”€ models/                # Output fine-tuned models
â”œâ”€â”€ train.py               # Training script
â”œâ”€â”€ infer.py               # Inference demo
â”œâ”€â”€ prompts/               # Prompt templates
â””â”€â”€ README.md
ğŸ™Œ Contribute Uzbek AI ğŸ‡ºğŸ‡¿
We're just getting started. If you're working with Uzbek NLP or LLMs â€” join us!

ğŸ§  Add new Uzbek datasets

ğŸ”¥ Share fine-tuned checkpoints

ğŸ’¬ Improve system prompts

ğŸ¤– Add inference UI (Streamlit / Telegram bot)

ğŸ Coming Soon
 Streamlit chat demo

 LoRA-based training

 HuggingFace Hub auto-push

 Uzbek QA dataset release

ğŸ“œ License
MIT â€” do whatever you want, just give credit.
Made with â¤ï¸ for O'zbek tilida AI rivoji uchun.

âœ¨ "Why wait for OpenAI to support Uzbek? Train your own."

yaml
Copy
Edit

---

## ğŸ’¡ Next Steps I Can Help With

- `train.py` and `infer.py` templates
- Streamlit UI for chatting
- HuggingFace Hub push setup (`transformers-cli`)
- Dataset format validators

Let me know if you want it zipped or deployed to GitHub â€” Iâ€™ll make it repo-ready ğŸš€








Ask ChatGPT
